---
title: 线性回归
tags:
  - 机器学习
categories: 机器学习
mathjax: true
abbrlink: 52662
date: 2022-03-11 15:19:49
---

# 线性回归



### 1. 线性回归的模型和损失函数

**线性回归问题：** 有 m 个样本，每个样本有 n 个特征和一个输出结果，$(x_{1}^{(i)},x_{2}^{(i)},\dots x_{n}^{(i)}), 其中 i = 1，2，3\dots m$ , 线性回归要解决的问题是对于一个新的 $(x_{1},x_{2},\dots,x_{n})$，它所对应的 $y$ 是多少，这里 $y$ 是连续的，所以这是一个回归问题，否则是一个分类问题。

如果这是一个线性模型，那么模型是这样的：

$h_{\theta}(x_{1},x_{2},\dots,x_{n}) = \theta_{0} + \theta_{1}x_{1}+\dots + \theta_{n}x_{n}$， 其中 $\theta_{i} (i=0,1,2,\dots n)$ 为模型参数，$x_{i} (i=1,2,3\dots n)$ 为每个样本的 n 个特征值，这个表示可以简化，我们增加一个特征 $x_{0} = 1$，这样就有 $h_{\theta}(x_{0},x_{1},\dots x_{n}) = \sum_{i=0}^{n}\theta_{i}x_{i}$  ，进一步用矩阵表示为 $h_{\theta}(X) = X\theta$ , 其中 $h_{\theta}(X)$ 是 mx1 的向量，$\theta$ 是 nx1 的向量，$X$ 是 mxn 维的矩阵，m 代表样本个数，n 代表样本的特征数 。

损失函数为：$J(\theta_{0},\theta_{1},\dots \theta_{n}) = \sum_{i=1}^{m}(h_{\theta}(x_{0}^{(i)},x_{1}^{(i)},\dots x_{n}^{(i)}) - y_{i})^2$ , 进一步用矩阵表示为 $J(\theta) = \frac{1}{2}(X\theta - Y)^T(X\theta - Y)$， 矩阵法表达更加简洁，后面就是用矩阵法来描述。


### 2. 线性回归算法的求解

​	对于线性回归算法的损失函数 $J(\theta) = \frac{1}{2}(X\theta - Y)^T(X\theta - Y)$，一般要求该损失函数的最小化时的参数 $\theta$ 有两种方法：第一种是**梯度下降法**，第二种是**最小二乘法**，求解方法如下：

1.  梯度下降法公式：

​		$\theta = \theta - \alpha X^T(X\theta - Y)$，通过迭代 逐渐逼近最优的 $\theta$ 

​	2. 最小二乘法公式：

​		$\theta = (X^TX)^{-1}X^TY$ 


### 3. 线性回归之多项式回归

​	前面介绍的线性回归的拟合函数是线性函数，是一次项的，如果这里不是一次项，而是多次的，那么这个模型就变成了多项式回归，例如，假设有一个只有两个特征的二次多项式回归模型：

​	$h_{\theta}(x_{1},h_{2}) = \theta_{0} + \theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2}+\theta_{5}x_{1}x_{2}$ 

一开始只有 $x_{1},x_{2}$ 两个特征，现在构造特征 $x_{1}^{2},x_{2}^{2},x_{1}x_{2}$ ，令他们分别为特征 $x_{3},x_{4},x_{5}$ ，这样就有了一个线性模型：

​	$h_{\theta}(x_{1},x_{2}) = \theta_{0} + \theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4}+\theta_{5}x_{5}$ 

​	通过改进特征，我们重新把不是线性回归的函数变回线性函数。


### 4. 线性回归之广义线性回归

​	上一节是对特征进行变化推广，这一节对 y 进行推广，如果 我们的 Y 和 X 并不是成线性的关系，但是 $lnY$ 和 $X$ 是线性的关系，模型为：

​	$ln(Y) = X\theta$

​	这样我们修改 Y，将其变为 $lnY$ ，从而仍然可以使用线性回归的算法去处理这个问题。

​	一般化： $g(Y) = X\theta $ 或者 $Y = g^{-1}(X\theta)$ 


### 5. 线性回归的正则化

​	为了防止过拟合，在建立线性回归模型时我们经常需要加入正则化，一般正则化分为 $L1$ 正则化和 $L2$ 正则化。

​	**Lasso回归** 

​	就是带 $L1$ 正则化项的 线性函数，和普通线性函数的区别就是在损失函数上增加了一个 $L1$ 正则化项。

​	损失函数为：

​	$J(\theta) = \frac{1}{2}(X\theta-Y)^T(X\theta-Y)+\alpha \left \| \theta  \right \|_{1}$ 

​	Lasso回归可以使得一些特征的系数变小，甚至还会使绝对值靠近 0 的参数变为 0，增强模型的泛化能力，因为参数过多容易过拟合，让某个参数变为 0 后，也就是减少了模型的参数。


​	**Ridge回归**

​	就是带 $L2$ 正则化项的 线性函数，和普通线性函数的区别就是在损失函数上增加了一个 $L2$ 正则化项。

​	损失函数为：

​	$J(\theta) = \frac{1}{2}(X\theta-Y)^T(X\theta-Y)+\frac{1}{2}\alpha\left \| \theta  \right \|_{2}^{2} $ ，Ridge 和 Lasso的区别是 Lasso 可能会使某些特征消失，Ridge则是在保留所有特征的情况下，缩小回归系数，使模型相对而言比较稳定。

​	使用最小二乘法求解：

​	令 $J(\theta)$ 的导数为 0 ，得到下式：

​	$X^T(X\theta - Y) + \alpha\theta = 0$

​	整理即得 $\theta = (X^TX + \alpha E)^{-1}X^TY$ ，其中 $E$ 为单位矩阵。 
