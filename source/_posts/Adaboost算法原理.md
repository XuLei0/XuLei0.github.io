---
title: Adaboost算法原理
abbrlink: 37516
date: 2022-03-17 18:56:19
tags: 机器学习
categories: 机器学习
mathjax: true
---

# Adaboost 算法原理

​	$Bboosting$ 算法的代表算法是 $Adaboost$ 算法，它即可用于分类，也可用作回归。

​	$Boosting$ 算法需要解决的问题：

1. 如何计算误差率
1. 如何学习弱学习器的权重系数
4. 如何更新样本权重
4. 使用何种结合策略



## 1. Adaboost 算法步骤

输入：训练数据集 $T={(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})}$，$y_{i}\in Y={-1,1}$，弱学习算法

输出：最终分类器 $G(x)$

（1）初始化训练数据的权值分布 $D_{1} = (w_{11},\dots,w_{1i},\dots,w_{1n})$，$w_{1i} = \frac{1}{N}$，$i=1,2,\dots,N$

​		  	**说明：**初始时训练数据集是均匀分布的，每个训练样本在基本分类器的学习中作用相同，在原始样本上学习基本分类器$G_{1}(x)$ 

（2）对 $m=1,2\dots,M$

​			（a）使用具有权值分布 $D_{m}$ 的训练数据集学习，得到基本分类器 $G_{m}(x)$

​			（b）计算 $G_{m}(x)$ 在训练数据集上的分类误差率

​					$e_{m} = P(G_{m}(x)\ne y_{i})=\sum_{i=1}^{N}w_{mi}I(G_{m}(x_{i})\ne y_{i})$

​					**说明：**$G_{m}(x)$ 在加权的训练数据集上的分类误差率是被$G_{m}(x)$误分类样本的权值之和		

​			（c）计算$G_{m}(x)$ 的系数

​					$a_{m} = \frac{1}{2}log\frac{1-e_{m}}{e_{m}}$ ，这里对数是自然对数

​					**说明：** $\alpha_{m}$表示$G_{m}(x)$在最终分类器的重要性，当$e_{m}\le\frac{1}{2}$ 时，$\alpha_{m}\ge 0$，并且$\alpha_{m}$随着$e_{m}$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大

​			（d）更新训练数据集的权值分布

​					$D_{m+1} = (w_{m+1,1},\dots,w_{m+1,i}\dots,w_{m+1,N})$

​					$w_{m+1,i} = \frac{w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i})),i=1,2\dots,N$

​					这里，$Z_{m}$ 是规范化因子：

​					$Z_{m} = \sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$ 

​					**说明：** 当 $G_{m}(x_{i})=y_{i}$，$w_{m+1,i} = \frac{w_{mi}}{Z_{m}}e^{-\alpha_{m}}$，当 $G_{m}(x_{i})\ne y_{i}$ ，$w_{m+1,i}=\frac{w_{mi}}{Z_{m}}e^{\alpha_{m}}$ ，由此可以看出被误分类的样本权值增大，被正确分类的样本权值减小，两相比较，误分类样本的权值被放大 $e^{2\alpha_{m}} = \frac{e_{m}}{1-e_{m}}$ 

（3）构建基本分类器的线性组合

​				$f(x) = \sum_{m=1}^{M}\alpha_{m}G_{m}(x)$

​			   得到最终分类器 

​				$G(x) = sign(f(x)) =sign\left( \sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right)$ 


## 2. Adaboost算法的解释

​		Adaboost算法可以被认为是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习算法。

### 2.1 前向分步算法

加法模型为：

$f(x) = \sum_{m=1}^{M}\beta_{m}b(x;\gamma_{m})$ 

其中 $b(x;\gamma_{m})$为基函数，$\gamma_{m}$ 为基函数的参数，$\beta_{m}$为基函数的系数

在给定训练数据及损失函数  $L(y,f(x))$ 的条件下，学习加法模型 $f(x)$ 就是最小化损失函数：

$min_{\beta_{m},\gamma_{m}}\sum_{i=1}^{N}L(y_{i},\sum_{m=1}^{M}\beta_{m}b(x_{i};\gamma_{m}))$

这是一个很复杂的优化问题，前向分步算法求解这一优化问题的想法是：因为学习的是加法模型，如果能从前往后，每一步只学习一个

基函数及其系数，逐步逼近优化目标函数，那么就可以简化优化的复杂度，具体来说，就是每步只需要优化如下损失函数：

$min_{\beta,\gamma}\sum_{i=1}^{N}L(y_{i},\beta b(x_{i};\gamma))$ 

前向分步算法的步骤：

输入：训练数据集$T={(x_{i},y_{i}),(x_{2},y_{2},\dots,(x_{N},y_{N}))}$，损失函数为$L(y,f(x))$，基函数集${b(x;\gamma)}$ 

输出：加法模型

（1）初始化 $f_{0}(x)=0$

（2）对 $m=1,2,\dots,M$

​			（a）极小化损失函数

​						$(\beta_{m},\gamma_{m}) = argmin_{\beta,\gamma}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+\beta b(x_{i};\gamma))$ , 得到参数 $\beta_{m}$，$\gamma_{m}$

​			（b）更新 

​						$f_{m}(x) = f_{m-1}(x)+\beta_{m}b(x;\gamma_{m})$ 

（3）得到加法模型

​						$f(x) = f_{M}(x) = \sum_{m=1}^{M}\beta_{m}b(x;\gamma_{m})$ 


### 2.2 前向分步算法与Adaboost

Adaboost算法是加法模型，学习算法为前向分步算法，损失函数为指数函数的分类问题。

模型为：		

$f_{k}(x) = f_{k-1}(x)+\alpha_{k}G_{k}(x)$

损失函数为：	

$argmin_{\alpha,G}\sum_{i=1}^{m}exp(-y_{i}f_{k}(x))$

$argmin_{\alpha,G}\sum_{i=1}^{m}exp[(-y_{i})(f_{k-1}(x)+\alpha G(x))]$

令 $w_{ki} = exp(-y_{i}f_{k-1}(x))$ ，它的值不依赖 $\alpha,G$，因此与最小化无关，代入得到：

$argmin_{\alpha,G}\sum_{i=1}^{m}w_{ki}exp[-y_{i}\alpha G(x)]$ 

第一步先求 $G_{k}(x)$

$\sum_{i=1}^{m}w_{ki}exp(-y_{i}\alpha G(x_{i})) =$

 $\sum_{y_{i}=G_{k}(x_{i})}w_{ki}e^{-\alpha} + \sum_{y_{i}\ne G_{k}(x_{i})}w_{ki}e^{\alpha} =$ 

$ (e^{\alpha}-e^{-\alpha})\sum_{i=1}^{m}w_{ki}I(y_{i}\ne G_{k}(x_{i})) + e^{-\alpha}\sum_{i=1}^{m}w_{ki}$ 

因为 $w_{ki}$ 和 $\alpha$ 和 $G$ 无关，所以 $G_{k}(x) = argmin_{G}\sum_{i=1}^{m}w_{ki}I(y_{i}\ne G(x_{i}))$ 

将 $G_{k}(x)$ 代入损失函数，并对 $\alpha$ 求导并令其为 0，可得：

$\alpha_{k} = \frac{1}{2}log\frac{1-e_{k}}{e_{k}}$ 

其中，$e_{k}$ 即为分类误差率：

$e_{k} = \frac{\sum_{i=1}^{m}w_{ki}I(y_{i}\ne G(x_{i}))}{\sum_{i=1}^{m}w_{ki}} = \sum_{i=1}^{m}w_{ki}I(y_{i}\ne G(x_{i}))$ 

样本权重更新：

利用 $f_{k}(x) = f_{k-1}(x) + \alpha_{k}G_{k}(x)$ 和 $w_{ki} = exp(-y_{i}f_{k-1}(x))$ 

$w_{k+1,i} = exp(-y_{i}f_{k}(x))$ 

$w_{k+1,i} = w_{ki}exp[-y_{i}\alpha_{k}G_{k}(x)]$

这样就得到了样本的权重更新公式


## 3. Adaboost回归问题的算法流程

Adaboost回归算法的变种有很多，这里介绍 Adaboost R2 回归算法过程。

输入：样本集 $T={(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{m},y_{m})}$，弱学习器算法，弱学习器迭代次数 $K$ 

输出：最终的强学习器 $f(x)$ 

（1）初始化样本集权重为

​				$D(1) = (w_{11},w_{12},\dots,w_{1m}); w_{1i}=\frac{1}{m}; i=1,2,\dots,m$

（2）对于 $k=1,2,\dots,K$

​			（a）使用具有权重 $D_{k}$ 的样本集来训练数据，得到弱学习器 $G_{k}(x)$

​			（b）计算训练集上的最大误差

​					 $E_{k} = max|y_{i}-G_{k}(x_{i})|$，  $i=1,2,\dots,m$ 

​			（c）计算每个样本的相对误差：

​					  线性误差：$e_{ki} = \frac{|y_{i}-G_{k}(x_{i})|}{E_{k}}$ 

​					  平方误差：$e_{ki} = \frac{(y_{i}-G_{k}(x_{i}))^{2}}{E_{k}^{2}}$

​					  指数误差：$e_{ki} = 1-exp(\frac{-|y_{i}-G_{k}(x_{i})|}{E_{k}})$

​			（d）计算回归误差率

​						$e_{k} = \sum_{i=1}^{m}w_{ki}e_{ki}$

​			（e）计算弱学习的系数

​						$\alpha_{k} = \frac{e_{k}}{1-e_{k}}$

​			（f）更新样本集的权重分布 

​						$w_{k+1,i} = \frac{w_{ki}}{Z_{k}}\alpha_{k}^{1-e_{ki}}$	

​						其中 $Z_{k} = \sum_{i=1}^{m}w_{ki}\alpha_{k}^{1-e_{ki}}$，为规范化因子

（3）构建最终强学习器

​			$f(x) = G_{k^{\ast}}(x)$，其中 $G_{k^{\ast}}(x)$ 是所有 $ln\frac{1}{\alpha_{k}},k=1,2\dots K$  的 中位数值乘以对应 $k^{\ast}$ 对应的弱学习器。


## 4. Adaboost算法的正则化

Adaboost算法也会产生过拟合，我们也可以在算法中加入正则化项，这个正则化通常叫做步长，定义为 $\nu$ ，之前弱学习器的迭代公式为：

​		$f_{k}(x) = f_{k-1}(x)+\alpha_{k}G_{k}(x)$

 加上正则化，就有：

​		$f_{k}(x) = f_{k-1}(x)+\nu\alpha_{k}G_{k}(x)$ 

$\nu$ 的取值范围为 $0<\nu\le1$，对于同样的训练集，较小的 $\nu$ 意味着需要更多的弱学习器进行迭代。	



## 5. 提升树

对于 Adaboost 算法，如果弱学习器是分类树或者回归树，那么就是提升树。

提升树模型实际上采用的是加法模型与前向分步算法，对分类问题采用的是二叉分类树，对回归问题采用的是二叉回归树。提升树模型

可以表示为决策树的加法模型：

$f_{M}(x) = \sum_{m=1}^{M}T(x;\theta_{m})$

其中，$T(x;\theta_{m})$ 表示决策树，$\theta_{m}$ 为决策树的参数，$M$ 为树的个数

对于二类分类问题，提升树算法只需将 $Adaboost$ 算法中的基本分类器换成二类分类树即可，其他一样。

主要介绍回归问题中的提升树算法

$f_{M}(x) = \sum_{m=1}^{M}T(x;\theta_{m})$ 

在前向分步的第 $m$ 步，给定当前模型 $f_{m-1}(x)$，需求解

$\theta_{m} = argmax_{\theta_{m}}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x_{i};\theta_{m}))$ 

如果损失函数是平方差损失函数：

$L(y,f_{m-1}(x)+T(x;\theta_{m}))=[y-f_{m-1}(x)-T(x;\theta_{m})]^{2}=[r-T(x;\theta_{m})]^{2}$

$r = y - f_{m-1}(x)$，我们称 $r$ 为当前模型拟合数据的残差，所以后面的决策树只需模拟当前模型的残差，就能得到最终的模型

回归问题的提升树算法：

输入：训练数据集 $T={(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{N},y_{N})}$

输出：提升树 $f_{M}(x)$ 

（1）初始化 $f_{0}(x) = 0$

（2）对 $m = 1,2,\dots,M$

​	  		（a）计算残差

​					 $r_{mi} = y_{i}-f_{m-1}(x_{i}),i=1,2,\dots,N$

​			  （b）拟合残差 $r_{mi}$ 学习一个回归树，得到 $T(x;\theta_{m})$

​		  	（c）更新 $f_{m}(x) = f_{m-1}(x) + T(x;\theta_{m})$

（3）得到回归问题的提升树

​				$f_{M}(x) = \sum_{m=1}^{M}T(x;\theta_{m})$ 
