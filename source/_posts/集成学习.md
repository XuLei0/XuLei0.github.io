---
title: 集成学习
tags: 机器学习
categories: 机器学习
mathjax: true

abbrlink: 8816
date: 2022-03-17 10:26:59
typora-root-url: ..
---

# 集成学习原理

​		集成学习是目前非常流行的机器学习方法，它本身不是一个单独的机器学习方法，而是通过构建并结合多个机器学习器来完成学习任务，可用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等。

集成学习通过训练出若干个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，集成学习主要需要解决两个问题：

* 如何得到若干个个体学习器
* 如何选择结合策略



## 1. 集成学习的个体学习器

如何得到若干个个体学习器？主要有两个方法：

* 所有个体学习器都是一个种类的，或称为同质的，比如都是决策树或者都是神经网络
* 所有个体学习器不都是一个种类的，或称为异质的，比如说可以同时学习出支持向量机学习器，逻辑回归学习器和朴素贝叶斯学习器

使用的最多的是同质学习器，而同质学习器中，个体学习器使用的最多的是 $CART$ 决策树和神经网络。

又可以按个体学习器之间是否存在依赖关系进行分类：

* 个体学习器之间存在强依赖关系，学习器之间需要串行生成，代表算法是 $boosting$ 系列算法
* 个体学习器之间不存在强依赖关系，学习器之间可以并行生成，代表算法是 $bagging$ 和随机森林系列算法




## 2. 集成学习之boosting

![Boosting算法](/images/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220317105033109.png)

​		$Boosting$ 算法的工作机制首先是按照初始化的训练样本权重（一般是平均权重）学习出一个弱学习器，然后根据弱学习器的学习误差率来更新训练样本的权重，使得弱学习器 1 分类错误的样本权重变高，在后面的学习器中便会重视这些样本，然后通过更新后的权重来训练弱学习器 2，依次进行，直到弱学习器达到事先指定的数目 $T$ ，最终将这 $T$ 个弱学习器通过结合策略进行整合，得到最终的强学习器。

​		$Boosting$ 系列算法里最著名的算法主要有 $AdaBoost$ 算法和 提升树系列算法，提升树算法里面使用的最多的是梯度提升树。



## 3. 集成学习之 bagging

Bagging算法中，弱学习器之间没有依赖关系，学习器可以并行生成

![Bagging算法](/images/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220317111004796.png)

Bagging中个体学习器的训练集是通过采样得到的，采样出 $T$ 个样本集，就能训练出 $T$ 个弱学习器，再通过结合策略得到强学习器。

这里的采样法一般使用自主采样法，即对 m 个样本的数据集，每次采取一个样本，然后放回后重新采样，这样采集 m 次，就能得到一个 m 

个样本的采样集，且一个样本集中可以包含多个一样的样本，也就是可以重复。


## 4. 集成学习之结合策略

​	**平均法**

​	对于回归预测问题，通常使用的结合策略是平均法，对弱学习器的输出进行平均得到最终的输出。

求平均时可以给每个弱学习器加权重

​	算术平均：

​	$H(x) = \frac{1}{T}\sum_{1}^{T}h_{i}(x)$

​	若每个学习器有个权重 $w$ ，则

​	$H(x) = \sum_{i=1}^{T}w_{i}h_{i}(x)$

​	所有弱学习器的权重相加等于 1

​	**投票法**

​	对于分类问题，通常使用的是投票法

* 相对多数投票法，即选最多的类别作为输出
* 绝对多数投票法，即不仅要最多，而且票数还要过半，没有过半拒绝预测
* 加权投票法，即每一个分类器都有个权重，最终输出各个分类器加权票数最高的类别

​	**学习法**

​	代表方法是 $stacking$ ，不是对弱分类器结果进行简单的逻辑处理，而是加一个学习器，将弱学习器的所有输出当作输入，将样本输出作为输出，也就是说每个弱学习器就是一个特征，最终得到一个学习器来得到最终结果



![image-20220513165044590](/images/集成学习/image-20220513165044590.png)



















