---
title: 梯度下降
tags:
  - 机器学习
categories: 机器学习
typora-root-url: ..
abbrlink: 45545
mathjax: true
date: 2022-03-09 16:28:38
---

# 梯度下降法

## 1. 梯度

**数学定义：** 在微积分里面，对一个多元函数的各个变量求偏导数，然后把各个变量的偏导数以向量的形式写出来，就是梯度。例如，对于函数 $f\left ( x,y \right )$ ,  它的梯度为$\left (  \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right)$，可以写为 $\nabla f\left ( x,y \right ) $ , 多个变量也是同理。

**梯度的几何意义：** 对于多元函数来说，某个点的梯度就是该函数在该点变化最快的方向。例如，对于函数 $f\left ( x,y \right )$ ，在点$\left ( x_{0},y_{0} \right )$ , 沿着梯度方向 $\left (  \frac{\partial f}{\partial x_{0}},\frac{\partial f}{\partial y_{0}}\right)$就是函数 f 增长最快的方向，容易获得函数最大值，-$\left (  \frac{\partial f}{\partial x_{0}},\frac{\partial f}{\partial y_{0}}\right)$就是函数 f 减少最快的方向，容易获得函数最小值。

**梯度下降：** 机器学习算法常用梯度下降法来最小化损失函数，通过一步步迭代，可以最小化损失函数以及求出模型的参数。



## 2. 梯度下降法算法

 **2.1 直观解释算法：**

损失函数的图像就好比如一座大山，我们要以怎样的方式走到山底就好比如要以怎样的方式找到损失函数的最小值。梯度下降法的思想是，每走到一个地方，都要计算这个点的坡度（梯度），就是该位置最陡峭的方向，然后往这个方向走，走到下一个位置，再计算这个位置最陡峭的方向，一直这样走下去直到自己觉得走到了一个平坦的位置，也许这个平坦的位置并不是山底，而是一个局部的山峰底处。

从上面可以看出，梯度下降不一定能够找到全局最优解，但是如果函数是凸函数，梯度下降法就一定能求出全局最优解。

![梯度下降直观的解释](/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTg2MTcwMA==,size_16,color_FFFFFF,t_70.png)



**2.2 梯度下降法的相关概念：**

1. 步长：步长就是每一次沿负梯度方向走的长度，用上面上山的例子来说，就是沿最陡峭的方向一次走的路程，步长过大不易找到损失函数最小值，步长过小虽然可以找到很近似于损失函数最小值的那个值，但是寻找效率低。
  
2. 特征：例如一个样本 $ \left ( x^{(0)},y^{(0)} \right )$，该样本特征为 $x^{(0)}$ ，输出为 $y^{(0)}$。
  
3. 假设函数：在监督学习中，为了拟合输入的样本而使用的假设函数。
  
4. 损失函数：是用来度量模型的拟合程度，损失函数极小化，说明拟合程度达到最好，所求模型的参数也即最优参数。



**2.3 梯度下降算法的描述：**

​	首先需要一个待优化模型的假设函数 $f(\theta)$ 和损失函数 $J(\theta)$ ，其中 $\theta = (\theta_{1},\theta_{2}\dots \theta_{n})$ 。


1. 确定当前位置损失函数的梯度，对于 $\theta_{i}$ 其梯度表达式为  $\frac{\partial}{\partial\theta_{i}}J(\theta_{0},\theta_{1}\dots\theta_{n})$ 。
  
2. 用步长乘以损失函数的梯度，即得到当前位置的下降距离，即 $\alpha\frac{\partial}{\partial\theta_{i}}J(\theta_{0},\theta_{1}\dots\theta_{n})$ 。
  
3. 确定所有 $\theta_{i}$ 梯度下降的距离都小于 $\varepsilon$ ，如果小于它，算法终止，当前的 $\theta$ 即为最终所求结果，否则进入步骤4 。
  
4. 更新所有的 $\theta$，公式为 ：$\theta_{i} = \theta_{i} - \alpha\frac{\partial}{\partial\theta_{i}}J(\theta_{0},\theta_{1}\dots\theta_{n})$ ，更新完后继续执行步骤 1 。

**2.4 梯度下降算法的调优：**

1. 算法的步长选择：步长的选择取决于数据样本，可以多取一些值，从大到小，用算法进行运行，看迭代的时候损失函数的变化，损失函数在减小说明取值有效，否则要增大步长。但是步长需要在一个适当的区间调整，步长过大可能会导致取不到最优解，步长过小，迭代速度会很慢，效率不高。
  
2. 算法参数的初始值选择：参数初始值的不同，会取到不同的最小值，因为对于非凸函数，梯度下降法只能求到一个局部最优解，所以需要使用不同的初值运行算法，以免求到局部最优解。
  
3. 归一化：由于数据中不同特征的取值范围可能相差很大，导致迭代过程缓慢，可以对特征数据归一化，也就是将特征进行如下处理$\frac{x-\bar{x}}{std(x)}$ ，其中 $\bar{x}$ 和 $std(x)$ 分别是该特征的平均值和标准差，这样该特征的期望就为 0 , 方差为 1，迭代速度就会得到加快。

​		![没有进行归一化损失函数的变化](/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/image-20220309192615536.png)![归一化后损失函数的变化](/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/image-20220309192631031.png)

上图中 $\theta_{1}$ 的范围远大于 $\theta_{2}$ 的范围，所以 $\theta_{2}$ 的变化对损失函数的影响会很大，$\theta_{1}$ 的变化对损失函数影响很小，就会导致迭代过程曲曲折折，迭代缓慢，右图是经过归一化后损失函数的变化，迭代速度明显会快。



## 3. 梯度下降法分类

**批量梯度下降法（BGD）**

就是所有样本都用于计算损失函数。


**随机梯度下降法（SGD）**

计算损失函数只使用一个随机样本来进行计算。

两者的比较：随机梯度下降法只用一个样本进行计算，训练速度很快，而批量梯度下降法在数据集很大的时候，训练速度慢，对于准确度而言，随机梯度下降法只使用一个样本计算梯度，可能求得的不是最优解，对于收敛速度来说，随机梯度下降法一次迭代就是用一个样本，导致迭代的方向变化频繁，不能很快收敛到局部最优解。


**小批量梯度下降法（MBGD）**

小批量梯度下降法是以上两种方法的一种折衷，只取一定数量的样本来进行计算损失函数。

