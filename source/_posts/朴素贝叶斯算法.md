---
title: 朴素贝叶斯算法
tags: 机器学习
categories: 机器学习
mathjax: true
abbrlink: 36559
date: 2022-03-16 15:53:13
---

# 朴素贝叶斯算法

​		朴素贝叶斯算法是基于贝叶斯定理和特征条件独立假设的分类算法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入 $x$ , 利用贝叶斯定理求出后验概率最大的类别作为输出，朴素贝叶斯算法实现简单，学习与预测效率都很高，是一种常用的方法。


## 相关统计学知识

​	1. 条件独立公式

​			$P(X,Y) = P(X)P(Y)$ 

2. 条件概率公式

​			$P(Y|X) = P(X,Y)/P(X)$

​			$P(X|Y) = P(X,Y)/P(Y)$ 

 3. 全概率公式

    $P(X) = \sum_{k}P(X|Y=Y_{k})$，其中 $\sum_{k}P(Y_{k})=1$

 4. 贝叶斯公式

    $P(Y_{k}|X) = \frac{P(X|Y_{k})P(Y_{k})}{\sum_{k}P(X|Y=Y_{k})P(Y_{k})}$ 

    

## 朴素贝叶斯的模型

朴素贝叶斯通过训练数据集学习联合概率分布 $P(X,Y)$，具体的学习：

**先验概率：** $P(Y=c_{k}), k = 1,2,\dots,K$

**条件概率：** $P(X=x|Y=c_{k}) = P(X^{(1)}=x^{(1)},\dots,X^{(n)}=x^{(n)}|Y=c_{k}), k=1,2,\dots,K$ 

根据以上概率分布就能够学习到联合概率分布 $P(X,Y)$ ，但是有个问题

条件概率 $P(X=x|Y=c_{k})$ 有指数级数量的参数，假设 $x^{(j)}$ 有 $S_{j}$ 个取值，$Y$ 可取值有 $K$ 个，那么参数的个数为 $K\prod_{j=1}^{n}S_{j}$ 

但是朴素贝叶斯法对条件概率分布作了条件独立性的假设，即：

$P(X=x|Y=c_{k}) = P(X^{(1)},\dots,X^{(n)}=x^{(n)}|Y=c_{k}) = \prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})$

朴素贝叶斯在分类时，对给定的输入 $x$ ，通过学习到的模型计算后验概率分布 $P(Y=c_{k}|X=x)$，将后验概率最大的类作为 $x$ 的类输出。

后验概率公式为：

$P(Y=c_{k}|X=x) = \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}P(X=x|Y=c_{k})P(Y=c_{k})}$ 

根据条件独立性假设又可得：

$P(Y=c_{k}|X=x) = \frac{P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k})}{\sum_{k}(P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k}))}$ 

于是朴素贝叶斯分类器表示为：

$y = f(x) = arg\max_{c_{k}}\frac{P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k})}{\sum_{k}(P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k}))}$ 

由于分母都是相同的，所以有：

$y = f(x) = arg\max_{c_{k}}P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k})$ 



## 朴素贝叶斯法的参数估计

朴素贝叶斯法中的学习意味着要估计 $P(Y=c_{k})$ 和 $P(X^{(j)}=x^{(j)}|Y=c_{k})$ ，可以应用极大似然估计法估计相应的概率。

$P(Y=c_{k})$ 的极大似然估计为：

$P(Y=c_{k}) = \frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N}, k = 1,2,\dots,K$

第 $j$ 个特征 $x^{(j)}$ 可能的取值的集合为 ${a_{j1},a_{j2},\dots,a_{js}}$，条件概率 $P(X^{(j)}=a_{jl}|Y=c_{k})$ 的极大似然估计为：

$P(X^{(j)}=a_{jl}|Y=c_{k}) = \frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=ajl, y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}, j=1,2,\dots,n;$    $l=1,2,\dots,S_{j};$     $k=1,2,\dots,K$  

上面的 $I$ 是 $0-1$ 损失函数：


## 朴素贝叶斯算法的流程

输入：训练数据 $T=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})\}$，$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值，$j=1,2,\dots,n$，$l=1,2,\dots,S_{j}$ 

输出：实例 $x$ 的分类

1. 计算先验概率以及条件概率：

​		$P(Y=c_{k}) = \frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N}, k = 1,2,\dots,K$

​		$P(X^{(j)}=a_{jl}|Y=c_{k}) = \frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=ajl, y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}, j=1,2,\dots,n;$ 

​	 2. 对于给定的实例计算：

​		$P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k}), k=1,2,\dots,K$ 

​	  3. 确定实例 $x$ 的类

​		$y = f(x) = arg\max_{c_{k}}P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_{k})$



## 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为 0 的情况，这时会影响到后验概率的计算结果，使分类误差产生偏差，解决这一问题是采用贝叶斯估计，具体的，

条件概率的贝叶斯估计是：

$P_{\lambda}(X^{(j)}=a_{jl}|Y=c_{k}) = \frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=ajl, y_{i}=c_{k})+\lambda}{\sum_{i=1}^{N}I(y_{i}=c_{k})+S_{j}\lambda}$ 

先验概率的贝叶斯估计是：

$P_{\lambda}(Y=c_{k}) = \frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda}{N+\lambda}$  





























