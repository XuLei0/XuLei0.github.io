---
title: 最小二乘法
tags:
  - 机器学习
categories: 机器学习
mathjax: true
abbrlink: 53030
date: 2022-03-10 19:48:05
---

# 最小二乘法

### 1. 最小二乘法介绍

​	对于线性函数，其损失函数是平方差损失函数，表达式如下：

​																$J(\theta) = \sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))^{2}$ 

最小二乘法就是求解 参数 $\theta$ 使得 $J(\theta)$ 最小，不同于梯度下降法，梯度下降法是通过迭代一步步逼近最小值，而最小二乘法是通过求解方程组求得一个确切的值。



### 2. 最小二乘法的代数解法

1. 假设线性函数的拟合函数表示为 $h_{\theta}(x1,x2,\dots x_{n}) = \theta_{0} + \theta_{1}x_{1} + \dots + \theta_{n}x_{n}$ ，其中 $\theta_{i}$ 为模型参数，$x_{i}$  为每个样本的 n 个特征值，这个表示可以简化为 $h_{\theta}(x1,x2,\dots x_{n}) = \theta_{0} + \theta_{1}x_{1} + \dots + \theta_{n}x_{n} = \sum_{i=0}^{n}\theta_{i}x_{i}$ 。
  
2. 损失函数表示为：$J(\theta_{0},\theta_{1},\dots,\theta_{n}) = \sum_{j=1}^{m}(h\theta(x_{0}^{(j)},x_{1}^{(j)},\dots,x_{n}^{(j)})-y^{(j)}) = \sum_{j=1}^{m}(\sum_{i=0}^{n}\theta_{i}x_{i}^{(j)}-y^{(j)})^{2}$ 。
  
3. 利用损失函数对 各个参数求偏导，并令导数为 0 可得：$\sum_{j=1}^{m}(\sum_{i=0}^{n}\theta_{i}x_{i}^{(j)}-y^{(j)})x_{i}^{(j)} = 0$ 。
  
4. 因为有 n+1 个参数，每个参数求一次偏导就有一个方程，故有 n+1 个方程，求解这个 n+1 元 一次方程，就可以得到所有的参数。



### 3. 最小二乘法的矩阵解法 
  1. 假设函数 $h_{\theta}(x_{1},x_{2},\dots x_{n}) = \theta_{0} + \theta_{1}x_{1} + \dots + \theta_{n}x_{n}$ 的矩阵表达式为：$h_{\theta}(X) = X\theta$ ，其中假设函数 $h_{\theta}(X)$ 为 mx1 向量，$\theta$ 为 \(n+1\)x1的向量，是参数向量，$X$ 为 mx\(n+1\) 维的矩阵，m 代表样本的个数，n 代表样本的特征数，n+1 多 1 是因为线性函数有一个截距。
    
  2. 损失函数定义为 $\frac{1}{2}(X\theta - Y)^T(X\theta - Y)$ ，其中 $Y$ 是样本的输出向量，维度为 mx1，$\frac{1}{2}$ 是为了求导后计算方便。
    
  3. 对 $\theta$ 求偏导，公式如下：$\frac{\partial}{\partial\theta}J(\theta) = X^T(X\theta - Y) = 0 $ 。
    
  4. 通过整理，$X^TX\theta = X^TY$ ，两边同时左乘 $(X^TX)^{-1}$ 可得：$\theta = (X^TX)^{-1}X^TY$ ，这样就求出了 参数向量 $\theta$ ，不用像代数法一个一个求导，而且还要求一个方程组。
    

### 4. 最小二乘法的局限性
  1. 最小二乘法需要计算 $X^TX$ 的逆，但是有可能它并不存在逆矩阵，这样就没有办法直接使用最小二乘法了，此时可以使用梯度下降法，当然 样本数据如果有很多冗余项的时候很容易出现 $X^TX$ 的行列式为 0 的情况，所以可以对数据进行预处理，去掉这个冗余特征，因为如果该特征冗余项很多，其实这个特征就不适合作为预测的依据。
    
  2. 当样本特征 n 非常大的时候，计算 $X^{T}X$ 的逆矩阵是一个非常耗时的工作，此时梯度下降法仍可以使用。
    
  3. 如果拟合的函数不是线性的，那么无法使用最小二乘法，此时梯度下降法仍然可以使用。
