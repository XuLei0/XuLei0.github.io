---
title: 梯度提升树(GBDT)算法原理
abbrlink: 44465
date: 2022-03-25 11:18:15
tags: 机器学习
categories: 机器学习
mathjax: true
---

# 梯度提升树（GBDT）算法

## 1. GBDT概述

​			Adaboost 是利用前一轮弱学习器的误差率来更新训练集的权重，这样一轮轮迭代下去。GBDT也是如此，也是用前向分步算法，但是弱学习器限定为 CART 树，同时迭代的方式也和 Adaboost 算法不同。

​			GBDT 对于前一轮得到的强学习器 $f_{t-1}(x)$，下一轮旨在要学习到一个弱学习器 $h_{t}(x)$ ，让本轮的损失函数 $L(y,f_{t}(x)) = L(y,f_{t-1}(x)+h_{t}(x))$ 最小，也就是说本轮迭代产生的 弱学习器，要让样本的损失函数尽可能的小。

​			**问题：** 损失函数各种各样，残差不好计算，损失函数的拟合不好度量。



## 2. GBDT 的负梯度拟合

第 $t$ 轮，第 $i$ 个样本的损失函数的负梯度表示为：

$r_{ti} = -[\frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}]_{f(x)=f_{t-1}(x)}$ 

$y_{i}$ 是样本的输出值，$f(x)$ 为 $t-1$ 轮学到的强学习器 

然后利用 $(x_{i},r_{ti}),i=1,2\dots m$ 我们可以拟合一棵 CART 回归树，得到第 $t$ 棵回归树，其对应的叶子节点区域为 $R_{tj},j=1,2\dots J$ , $J$ 为

叶子结点的个数  

对每一个叶子结点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值 $c_{tj}$ 如下：

$c_{tj} = argmin_{c}\sum_{x_{i}\in R_{tj}}L(y_{i},f_{t-1}(x)+c)$

之前在求回归问题时，使损失函数最小的那个值 $c_{tj}$ 是叶子结点里所有样本的平均值（平方差损失函数），现在损失函数普遍化，不一定是

平方差损失函数

所以得到本轮的决策树拟合函数为：

$h_{t}(x) = \sum_{j=1}^{J}c_{tj}I(x\in R_{tj})$

所以本轮最终得到的强学习器表示为：

$f_{t}(x) = f_{t-1}(x) + \sum_{j=1}^{J}c_{tj}I(x \in R_{tj})$



## 3. GBDT 回归算法

输入：训练集样本，$T((x_{1},y_{1}),(x_{2},y_{2}),\dots(x_{m},y_{m}))$ ，最大迭代次数 $T$ , 损失函数 $L$ 

输出：强学习器 $f(x)$

（1）初始化弱学习器

​			$f_{0}(x) = argmin_{c}\sum_{i=1}^{m}L(y_{i},c)$

（2）迭代轮数 $t=1,2\dots T$，有

​			a) 对样本 $i=1,2,\dots m$ ，计算负梯度

​				$r_{ti} = -[\frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}]$

​			b) 利用 $(x_{i},r_{ti}), i=1,2\dots m$ ，拟合一棵 CART 回归树，其对应的叶子区域 为 $R_{tj},j=1,2\dots,J$ ，其中 $J$ 为叶子结点个数

​			c) 计算每个叶子结点区域的最佳拟合值

​				$c_{tj} = argmin_{c}\sum_{x_{i}\in R_{tj}}L(y_{i},f_{t-1}(x_{i})+c)$ 

​			d) 更新强学习器 

​				$f_{t}(x) = f_{t-1}(x) + \sum_{j=1}^{J}c_{tj}I(x\in R_{tj})$

​	 （3）得到强学习器 $f(x)$ 的表达式

​				$f(x) = f_{T}(x) = f_{0}(x) + \sum_{t=1}^{T}\sum_{j=1}^{J}c_{tj}I(x\in R_{tj})$ 


## 4. GBDT分类算法

​			GBDT 的分类算法思想上和GBDT回归算法一样，但是分类问题的样本输出是离散值，无法去拟合类别输出的误差。

解决这个问题，主要有两个方法：

* 使用指数损失函数，此时的 GBDT 就是 Adaboost 算法
* 利用对数似然函数，使用类别预测的概率值和真实概率值的差来拟合损失，分为 二元分类和多元分类
  

### 4.1 二元 GBDT 分类算法

​		损失函数为：

​		$L(y,f(x)) = log(1+exp(-yf(x)))$

​		其中 $y\in\{-1,+1\}$，则此时负梯度误差为：

​		$ r_{ti} = -\left[ \frac{\partial L(y,f(x_{i}))}{\partial f(x_{i})} \right ]_{f(x)=f_{t-1}(x)}= \frac{y_{i}}{(1+e^{y_{i}f(x)})} $  

​		对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：

​		$c_{tj} = argmin_{c}\sum_{x_{i}\in R_{tj}}log(1+exp(-y_{i}(f_{t-1}(x_{i})+c)))$	

​		上面的式子优化比较难，所以取近似值：

​		$c_{tj} = \sum_{x_{i}\in R_{tj}}r_{ti}/\sum_{x_{i}\in R_{tj}}|r_{ti}|(1-|r_{ti}|)$ 


### 4.2 多元 GBDT 分类算法

​		损失函数为：

​		$L(y,f(x)) = -\sum_{k=1}^{K}y_{k}log(p_{k}(x))$ 

​		样本输出类别为 k，那么 $y_{k}=1$，其余类别 $y_{k}=0$，类别 $k$ 的概率 $p_{k}(x)$ 的表达式为：

​		$p_{k}(x) = exp(f_{k}(x))/\sum_{l=1}^{K}exp(f_{l}(x))$ 

​		根据以上两式求得负梯度误差为：

​		$r_{til} = -\left [\frac{\partial L\left(y_{i},f(x_{i})\right)}{\partial f(x_{i})}\right]_{f_{k}(x)=f_{l,t-1}(x)}=y_{il}-p_{l,t-1}(x_{i})$ 

​		误差其实就是样本 $i$ 对应类别 $l$ 的真是概率和 $t-1$ 轮预测值的差值

​		对于生成决策树的叶子结点，最佳的负梯度拟合值为：

​		$c_{tjl} = argmin_{c_{jl}}\sum_{i=0}^{m}\sum_{k=1}^{K}L(y_{k},f_{t-1,l}(x)+\sum_{j=0}^{J}c_{jl}I(x_{i}\in R_{tjl}))$ 

​		可用下式近似替代：

​		$c_{tjl} = \frac{K-1}{K}\frac{\sum_{x_{i}\in R_{tjl}}r_{til}}{\sum_{x_{i}\in R_{til}}|r_{til}|(1-|r_{til}|)}$

​				







































​				 















 
