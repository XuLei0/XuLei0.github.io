---
layout: 'n'
title: 逻辑回归
date: 2022-03-12 16:16:50
tags:
    - 机器学习
categories: 机器学习
mathjax: true
---

# 逻辑回归

## 1. 从线性回归到逻辑回归

​	线性回归求的是输出特征向量 Y 和输入样本矩阵 X 之间的线性关系 $\theta$ ，满足 $Y = X\theta$ 。此时我们的 Y 是连续的，所以是回归模型，如果我要想要 Y 是离散值，则可以再对 Y 进行一次转换，变为 $g(Y)$，如果  $g(Y)$ 的值在某个区间内是类别 A，在另一个区间内是 B，这样就得到了一个分类模型。如果类别个数为 2，那么就是一个二分类模型。


## 2. 二元逻辑回归模型

​	逻辑回归模型就是在线性模型的基础之上加一个函数转换为离散值，这个函数在逻辑回归里面是 sigmod 函数：$g(z) = \frac{1}{1+e^{-z}}$ ，为什么是这个函数：

 * 当 $z$ 趋近于正无穷时，$g(z)$ 趋于 1 ，而当 z 趋于负无穷时，$g(z)$ 趋于0，这个性质很适合用来进行分类。
 * sigmod 函数还有一个很好的导数性质，$g^{'}(z) = g(z)(1-g(z))$ 
   

逻辑回归模型：

​	$h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}$ 

$x$ 为样本输入，$h_{\theta}(x)$ 为模型输出，可以理解为某一类别的概率，而 $\theta$ 为模型参数，对于二元分类，如果 $h_{\theta}(x) > 0.5$，即 $x\theta > 0$，那么 y 预测为 1，如果 $h_{\theta}(x) < 0.5$ ，即 $x\theta < 0$，那么 y 预测为 0 。


逻辑回归的矩阵形式：

$h_{\theta}(X) = \frac{1}{1+e^{-X\theta}}$ , $h_{\theta}(X)$ 是模型输出，是 mx1 的维度，$X$ 是特征矩阵，是 mxn 的维度，$\theta$ 是分类模型的待求系数，是 nx1 的维度。


 ## 3. 二元逻辑回归的损失函数

​	线性回归的输出是连续值，所以可以使用平方差来定义损失函数，但是逻辑回归的输出不是连续值，所以平方差损失函数不适合逻辑回归，不过可以使用最大似然法推导出损失函数。

​	对于二元逻辑回归，假设输出 y 只有 0 和 1两类。

​	$P(y=1|x,\theta) = h_{\theta}(x)$

​	$P(y=0|x,\theta) = 1-h(\theta)$

写成一个函数就是：$P(y|x,\theta) = h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}$ ，y 只能取 0 和 1 。

通过似然函数最大化来求解参数 $\theta$ ，进行转换，将似然函数取对数并取反作为损失函数，然后最小化损失函数。

似然函数：$L(\theta) = \prod_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}$

损失函数：$J(\theta) = -lnL(\theta) = -\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))$   

损失函数的矩阵表式：$J(\theta) = -Y^Tlogh_{\theta}(X) - (E-Y)^Tlog(E-h_{\theta}(X))$，其中 E 为全 1 向量。


## 4. 二元逻辑回归损失函数的优化方法

​	这里只介绍梯度下降法

$\frac{\partial}{\partial\theta}J(\theta) = X^T[\frac{1}{h_{\theta}(X)}\odot h_{\theta}(X)\odot(E-h_{\theta}(X))\odot(-Y)] + X^T[\frac{1}{E-h_{\theta}(X)}\odot h_{\theta}(X)\odot(E-h_{\theta}(X))\odot(E-Y)]$ 

简化为：

$\frac{\partial}{\partial\theta}J(\theta) = X^T(h_{\theta}(X)-Y)$

迭代公式为：

$\theta = \theta - \alpha X^{T}(h_{\theta}(X) - Y)$ , $\alpha$ 为步长 。


## 5. 二元逻辑回归的正则化

​	逻辑回归也会有过拟合的问题，也需要考虑正则化，常见的有 $L1$ 正则化 和 $L2$ 正则化。

$L1$正则化

$J(\theta) = -Y^Tlogh_{\theta}(X) - (E-Y)^Tlog(E-h_{\theta}(X)) + \alpha\left \| \theta \right \|_{1}$ 

$L2$ 正则化

$J(\theta) = -Y^Tlogh_{\theta}(X) - (E-Y)^Tlog(E-h_{\theta}(X)) + \frac{1}{2}\alpha\left \| \theta \right \|_{2}^{2}$ 


## 6. 多元逻辑回归

对于二元逻辑回归：

​	$P(y=1|x,\theta) = h_{\theta}(x) = \frac{1}{1+e^{-x\theta}} = \frac{e^{x\theta}}{1+e^{x\theta}}$

​	$P(y=0|x,\theta) = 1-h(\theta) = \frac{1}{1+e^{x\theta}}$ 

​	两则相除取对数

​	$ln\frac{P(y=1|x,\theta) }{P(y=0|x,\theta)} = x\theta$ 
​	

推广到多元，假设 y 的值可以取：$1,2,3\dots K$

$ln\frac{P(y=1|x,\theta) }{P(y=K|x,\theta)} = x\theta_{1}$

$ln\frac{P(y=2|x,\theta) }{P(y=K|x,\theta)} = x\theta_{2}$ 

$\dots$

$ln\frac{P(y=K-1|x,\theta) }{P(y=K|x,\theta)} = x\theta_{K-1}$

$\sum_{i=1}^{K}P(y=i|x,\theta) = 1$

上面有 K 个方程，求解该方程得：

$P(y=k|x,\theta) = \frac{e^{x\theta_{k}}}{1+\sum_{t=1}^{K-1}e^{x\theta_{t}}}, k = 1,2,\dots K-1$

$P(y=k|x,\theta) = \frac{1}{1+\sum_{t=1}^{K-1}e^{x\theta_{t}}}$ 

多元逻辑回归的损失函数和优化方法和二元逻辑回归类似 。





























  
