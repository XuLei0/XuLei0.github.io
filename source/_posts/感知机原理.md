---
title: 感知机原理
tags:
  - 机器学习
categories: 机器学习
mathjax: true
abbrlink: 7270
date: 2022-03-14 14:21:07
---

# 感知机原理

​		感知机模型虽然在今天看来它的泛化能力不强，但是它的原理却很值得研究，可以为后面的支持向量机、神经网络做一个铺垫。

## 1. 感知机模型

​	**思想**：即在高维空间中找到一个超平面能够将所有的数据样本分开。
​	**线性可分性**：如果这个超平面找不到，也就意味着数据是线性不可分的，反之则是线性可分的。这是感知机的缺点，支持向量机在面对线性不可分的情况时，可以采用核技巧来让数据高维可分，神经网络可以通过激活函数和增加隐藏层的方式来让数据线性可分。

​	**数学描述：**
​		对于一些数据样本 $(x_{1}^{(i)},x_{2}^{(i)},\dots x_{n}^{(i)},y_{i}),i=1,2,\dots n$ , 感知机的目标是找到一个超平面，即$\theta_{0} + \theta_{1}x_{1} + \dots +\theta_{n}x_{n} = 0$ ，让其中一种类别的样本满足 $\theta_{0} + \theta_{1}x_{1} + \dots +\theta_{n}x_{n} > 0$，另一种类别的样本满足 $\theta_{0} + \theta_{1}x_{1} + \dots +\theta_{n}x_{n} < 0$，如果数据线性可分，这样的超平面一般不唯一，也就是说感知机模型可以有多个解。

​		为了简化超平面的写法，我们增加一个特征 $x_{0} = 1$，这样超平面为$\sum_{i=0}^{n}\theta_{i}x_{i} = 0$，用向量来表示超平面为 $\theta\cdot x = 0$，其中 $\theta$ 是(n+1)x1维的，x 是 (n+1)x1 的向量。而感知机模型就是：$y = sign(\theta\cdot x)$，其中 sign 是符号函数 。


## 2. 感知机模型的损失函数

​		损失函数是所有误分类点到超平面的距离总和。

​		点 $x_{0}$ 到某个平面上的距离公式：$d = \frac{1}{\left \| \theta \right \|}|\theta\cdot x_{0}|$ ，这里 $\left \| \theta \right \|$ 是 $L2$ 范数。

​		对于误分类点$(x_{i},y_{i})$来说满足这个式子： $-y_{i}(\theta\cdot x_{i}) > 0$ ，这是因为当 $\theta\cdot x>0$ 时，$y_{i} = -1$，而当 $\theta\cdot x<0$ 时，$y_{i} = +1$ , 因此误分类点到超平面的距离为：$-\frac{1}{\left \| \theta \right \|}y_{i}(\theta\cdot x_{i})$  。

​		设误分类点的集合为 $M$，那么所有误分类点到超平面的距离总和为：$-\frac{1}{\left \| \theta \right \|}\sum_{x_{i}\in M} y_{i}(\theta\cdot x_{i})$ 。不考虑 $\frac{1}{\left \| \theta\right \|}$ , 就得到了感知机模型的损失函数：$L(\theta) = -\sum_{x_{i}\in M}y_{i}(\theta\cdot x_{i})$，其中 $M$ 为误分类点的集合。



## 3. 感知机模型损失函数的优化方法

​		这个损失函数可以使用梯度下降法或者拟牛顿法来解决，常用的是梯度下降法。

​		由于只有误分类点才能参与损失函数的计算，所以优化方法采用**随机梯度下降法**来进行优化。

​		每次只用一个误分类样本点来计算梯度，假设这个误分类样本点是 $(x_{i},y_{i})$，则梯度下降公式为：

​		$\theta = \theta + \alpha y_{i}x_{i}$ ，其中 $\alpha$ 为步长，$y_{i}$ 为样本输出 1 或者 -1。


## 4. 感知机模型的原始形式

​		算法的输入：$(x_{1}^{(i)},x_{2}^{(i)},\dots x_{n}^{(i)},y_{0})$ 

​		算法的输出：分离超平面的模型系数 $\theta$ 向量		

​		**执行步骤：**

1. 定义所有 $x_{0}$ 为 1，选择向量 $\theta$ 的初始值和步长 $\alpha$ , 可以将 $\theta$ 初始化为 0 向量，步长设置为 1 ，感知机模型不唯一，这两个初值不同会影响到最终的迭代结果。
2. 在训练数据集里面选择一个误分类点 $(x_{1}^{(i)},x_{2}^{(i)}\dots x_{n}^{(i)},y_{i})$ ，这个点要满足：$y_{i}\theta\cdot x_{i} <= 0$ 。
3. 对 $\theta$ 向量进行一次随机梯度下降迭代：$\theta = \theta + \alpha y_{i}x_{i}$ 。
4.  检查训练集中是否还有误分类点，如果没有算法结束，如果有，跳转到步骤 2 继续执行。



## 5. 感知机模型的对偶形式

​		对偶形式是对算法执行速度的优化，具体如何优化如下：

​			根据原始形式的迭代公式 $\theta = \theta + \alpha y_{i}x_{i}$ 可以看出，我们每次梯度迭代都是选取一个样本来更新 $\theta$ 向量，最终经过若干次的迭代得到最终的结果。对于从来没有被误分类过的样本，它的迭代次数为 0 , 对于被多次误分类过的样本 $j$ , 它参与 $\theta$ 更新的次数设为 $m_{j}$ ，如果 $\theta$ 向量刚开始就初始化为 0 向量，经过多次迭代后 $\theta$ 向量可以表达为：
​		$\theta = \alpha\sum_{j=1}^{m}m_{j}y_{j}x_{j}$ 
​		其中 $m_{j}$ 为样本 $j$ 被误分类的次数，每个样本的 $m_{j}$ 初始值为 0 ，样本 $j$ 每次被误分类时，$m_{j}$ 的值都加 1 。

​		由于步长 $\alpha$ 为常量，我们令 $\beta_{j} = \alpha m_{j}$，这样 $\theta$ 的表达式就为：
​		$\theta = \sum_{j=1}^{m}\beta_{j}y_{j}x_{j}$ 

​			在每一次判断误分类点的时候，本来判断公式为 $y_{i}(\theta\cdot x_{i}) < 0$，现在对偶形式使用如下公式：$y_{i}\sum_{j=1}^{m}\beta_{j}y_{j}x_{j} \cdot x_{i}$ ，**注意** 这里的 $x_{j}$ 和 $x_{j}$ 是内积运算，而这个内积运算在下面的迭代中重复计算，如果在优化前就将内积运算计算完毕并保存下来，优化过程就会更快，省下很多时间，这也就是为什么对偶形式比一般的形式要更优的原因。

​			样本的内积矩阵称为 Gram矩阵 ，它是一个对称矩阵，记为 $G = [x_{i} \cdot x_{j}]$ 
​		如果 $(x_{i},y_{i})$ 是误分类样本点，则更新 $\beta_{i} = \beta_{i} + \alpha$
​		直到没有误分类点，算法执行结束，$\theta = \sum_{j=1}^{m}\beta_{j}y_{j}x_{j}$ 

**整体步骤：**
	1. 定义所有的 $x_{0}$ 为 1 ，步长 $\alpha$ 初值 ，可以将 $\beta$ 设为 0，$\alpha$ 设为 1 。
	2. 计算所有样本内积形成的 Gram 矩阵 G 。
	3. 在训练集中选择一个误分类点 $(x_{i},y_{i})$ ，这个点应满足：$y_{i}\sum_{j=1}^{m}\beta_{j}y_{j}x_{j} \cdot x_{i} <=0$ ，通过查询 Gram 矩阵的 $g_{ij}$ 的值来快速计算是否小于 0 。
	4. 对 $\beta$ 向量的第 i 个分量进行一次更新：$\beta_{i} = \beta_{i} + \alpha$ 
	5. 检查训练集中是否还有误分类点，没有则算法结束，如果有则跳转到步骤 3 继续执行。























