---
title: 决策树算法原理
tags:
  - 机器学习
categories: 机器学习
mathjax: true
abbrlink: 1025
date: 2022-03-14 18:32:13
---

# 决策树算法原理

## 1. 信息论基础

​		**决策树的思想：** 类似于 if - else 语句，但是决策树可选择的条件很多，于是就有一个这个问题，有这么多条件，到底优先选择哪个条件呢？如何衡量条件选择的优劣是决策树的关键，接下来介绍的信息论中的熵就是解决这个问题。

​		**熵的概念：** 熵度量了事物的不确定性，越不确定的事物，它的熵越大。随机变量 $X$ 的熵的表达式为：

​		$H(X) = -\sum_{i=1}^{n}p_{i}log p_{i}$

​		其中 n 代表 $X$ 的 n 种不同的离散值，$p_{i}$ 代表了 $X$ 取值为 i 的概率，$log$ 是以 2 或者 $e$ 为底 

​		**联合熵：** $H(X,Y) = -\sum_{x_{i}\in X}\sum_{y_{i}\in Y}p(x_{i},y_{i})logp(x_{i},y_{i})$ 

​		**条件熵：** 度量了我们在知道 $Y$ 以后，$X$ 剩下的不确定性。表达式为：

​		$H(X|Y) = \sum_{j=1}^{n}p(y_{j})H(X|y_{j})$ ，其中 n 表示 $Y$ 可以取值的个数

​		**互信息（信息增益）：** $X$ 在知道了 $Y$ 以后不确定性的减少程度，记为 $I(X,Y)$ ，在决策树中这个叫信息增益，$ID3$ 算法就是用信息

增益来判断当前结点应该用什么特征来构建决策树，信息增益大，则越适合用来分类。



## 2. 决策树 ID3 算法

​	**算法输入**：m 个样本，样本输出集合为 $D$ ，每个样本有 n 个离散特征，特征集合即为 $A$。

​	**算法输出：** 决策树 $T$ 

​	**算法流程：** 

1. 初始化信息增益的 阈值 $\epsilon$ 
2. 判断样本是否都属于同一类输出 $D_{i}$，如果是则返回单节点树 $T$，标记为 $D_{i}$ 

2. 判断剩余可待选择的特征是否为空，如果是则返回单节点树 $T$,  标记为样本中输出类别 $D$ 实例最多的那个类别
3. 计算 $A$ 中的各个特征对输出 $D$ 的信息增益，选择信息增益最大的特征类别 $A_{g}$
4. 如果 $A_{g}$ 的信息增益小于阈值 $\epsilon$ ，则返回单节点树 $T$ , 标记类别为样本输出类别最多的类别
5. 否则，按特征 $Ag$ 的不同取值 $Ag_{i}$ 将对应的样本输出 $D$ 分成不同的类别 $D_{i}$，每个类别只产生一个子节点，对应的特征为 $A_{gi}$，返回增加了节点的树 $ T $
6. 对于所有的子节点 ，令 $D = D_{i}$,  $A = A-{A_{g}}$，递归调用 2 - 6 步，得到字数  $T_{i}$ 


## 3. 决策树 ID3 算法的不足

1. $ID3$ 算法没有考虑连续特征，所以连续值无法在 $ID3$ 中运用，这很大程度上限制了 $ID3$ 的用途
2. $ID3$ 采用信息增益大的特征优先建立决策树结点，但其实有个缺陷，在相同条件下，取值比较多的特征会比取值比较少的特征信息增益大
3. $ID3$ 没有对缺失值的情况做考虑
4. 没有考虑过拟合问题
  


## 4. 决策树 C4.5 算法

​		$C 4.5$ 算法是对 $ID3$ 算法的一个改进，解决了 $ID3$ 算法的不足之处 。

​		**处理连续特征：**

​			m 个样本，其某个特征 $A$ 有 m 个取值，将这 m 个值从小到大排列为 $a_{1},a_{2},\dots a_{m}$，取相邻两个数据的平均值，一共可以取到 m-1 个划分点，其中第 i 个划分点 $T_{i}$ 表示为：$T_{i} = \frac{a_{i}+a_{i+1}}{2}$ ，对于这 m-1 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。例如，取到的增益最大点为 $a_{t}$，则小于 $a_{t}$ 的值为类别 1，大于 $a_{t}$ 的值为类别 2 ，这样就做到了连续特征离散化 。但是不同于离散特征，连续特征在后面的选择中还可以参与子节点的选择过程 。

​		**信息增益比：**

​			信息增益容易偏向于取值较多的特征，对于这个问题，引入一个信息增益比的概念，$I_{R}(X,Y)$，它是信息增益和特征熵的比值，表达式如下：
​																	      $I_{R}(D,A) = \frac{I(A,D)}{H_{A}(D)}$

其中 $D$ 为样本特征输出的集合，$A$ 为样本特征，对于特征熵 $H_{A}(D)$，表达式如下：

​														          $H_{A}(D) = -\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}$

其中 n 为特征 A 的类别数，$D_{i}$ 为特征 A 的 第 i 个取值对应的样本数。$|D|$ 为总的样本数。

**缺失值的处理和过拟合问题这里不做介绍**




## 5. 决策树 $C4.5$ 算法的不足

1. 容易过拟合，需要进行剪枝，思路主要有预剪枝和后剪枝，后剪枝即先生成决策树，再通过交叉验证来剪枝。
2. $C4.5$ 是多叉树，在很多时候，计算机中的二叉树模型会比多叉树运算效率高。
3. $C4.5$ 只能用于分类，不能用于回归。
4. $C4.5$ 使用了熵模型，里面有大量的 对数运算，如果是连续特征还有大量的排序运算。
  

这 4 个问题在后面的 $CART$ 树里面进行了改进，scikit-learn 的决策树就是使用的 $CART$ 算法。



## 6. CART 算法

### CART 树选择最优特征的方法 

$ID3$ 算法中使用信息增益来选择特征，$C4.5$ 中选择信息增益比来选择特征，这两者都是基于信息论的熵模型的，涉及大量的对数运算。

$CART$ 算法使用的是基尼系数，以此来替代前两者中的信息增益，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越

好，和信息增益（比）相反，信息增益（比）是越大越好。

**基尼系数：**

假设一个随机变量有 $K$ 个取值，第 $K$ 个取值的概率为 $p_{k}$，则基尼系数的表达式为：

$Gini(p) = \sum_{k=1}^{K}p_{k}(1-p_{k}) = 1 - \sum_{k=1}^{K}p_{k}^2$ 

如果是二分类问题，基尼系数表达式为：

$Gini(p) = 2p(1-p)$ 

对于给定的样本 $D$，假设有 $K$ 个类别，第 k 个类别的数量为 $C_{k}$，则样本 $D$ 的基尼系数表达式为：

$Gini(D) = 1 - \sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}$

类似于条件熵，如果根据特征 $A$ 将样本 $D$ 分成了 $D1$ 和 $D2$，则在特征 $A$ 的条件下，D 的基尼系数为：

$Gini(D,A) = \frac{|D1|}{|D|}Gini(D1) + \frac{|D2|}{D}Gini(D2)$



### CART算法对连续特征和离散特征的改进

**连续特征：** 和 $C4.5$ 基本一样，唯一改变的地方就是 $CART$ 算法是使用基尼系数（方差）来选择特征。还要注意：连续特征在后面的选择中仍然可

以参与子节点的产生过程。

**离散特征：** 在 $ID3$ 和 $C4.5$ 中，某个特征被选取后，如果他有 n 个取值，就会在决策树上建立 n 个子节点，这导致了决策树是一个多叉

树。但是 $CART$ 算法不同，它是不停的二分，例如，一个特征 $A$ , 它有 3 个 类别分别是 $A_{1},A_{2},A_{3}$，$CART$ 算法会考虑以下几种情况：

* $\{A_{1}\}$和 $\{A_{2},A_{3}\}$
* $\{A_{2}\}$ 和 $\{A_{1},A_{3}\}$
* $\{A_{3}\}$ 和 $\{A_{1},A_{2}\}$ 

找到基尼系数最小的组合，假设是 第一组 $\{A_{1}\}$ 和 $\{A_{2},A_{3}\}$ , 那么建立两个结点，一个是 $\{A_{1}\}$ , 一个是 $\{A_{2},A_{3}\}$ , 由于这次没有将 特征

$A$ 的取值完全分开，所以在后面选择特征的时候还可以继续选择特征 $A$ 来将 $A_{2}$ 和 $A_{3}$ 分开，这与前两个算法不同。



### CART 分类树建立的算法流程

算法输入：训练集 $D$ ，基尼系数的阈值，样本个数阈值

算法输出：决策树 $T$

1.  对于当前结点的数据集 $D$，如果样本个数小于阈值或者没有特征，则返回决策树，停止递归。
2.  计算样本集 $D$ 的基尼系数，如果基尼系数小于阈值，则返回决策树，停止递归。
3.  计算当前结点现有剩余特征及其各个特征值对数据 $D$ 的基尼系数，连续值有连续值的处理方式，离散值有离散值处理的方式。
4. 选择基尼系数最小的特征 $A$ 和对应的特征值 $a$ ，根据这个最优特征和特征值，把数据集划分成两部分 $D1$ 和 $D2$，同时建立两个结点，左节点数据集为为 $D1$ ，右节点数据集为 $D2$。
5.  然后递归的执行 1-4 步，生成决策树。

预测时，某个样本落入某个结点时，预测类别为该结点中概率最大的类别。



### CART 回归树建立算法

​	回归树和分类树大部分地方都是类似的，这里主要介绍不同的地方。

**特征选取的方式不同：**

做分类时采用的是基尼系数来选择特征，那是因为样本输出是离散值，可以计算概率，但是回归时，样本输出是离散值。回归树使用常见的

方差度量方式，样本数据不论是连续的还是离散的，我们要找到的是一个最优的切分变量 $x^{(j)}$（特征）以及切分点（特征值）, 它会把样本集 $D$ 

划分为两个区域：

$R_{1}(j,s) = \{ x|x^{(j)}\le s\}$   和  $R_{2}(j,s)=\{x|x^{(j)}>s \}$ 

求解：

$ min_{j,s}[min_{c_{1}}\sum_{x_{i}\in R_{1}(j,s)}(y_{i}-c_{1})^{2} + min_{c_{2}}\sum_{x_{i}\in R_{2}(j,s)}(y_{i}-c_{2})^2]$  ，其中 $ c_{1}$ 和 $c_{2}$ 就是 $R_{1}$ 和 $R_{2}$ 区域所有样本输出的平均值。也就是

找特征以及特征值，让划分后两个区域样本输出的平均值之和最小。

**决策树建立后做预测的方式不同：**

​	上面介绍的分类树是使用概率最大的类别作为当前结点的预测类别，而回归树是使用结点中样本的均值或者中位数来预测输出结果。


### CART 树的剪枝算法

决策树算法很容易过拟合，导致泛化能力差，为了解决这个问题，需要对 $CART$  树进行剪枝。	

分类树和回归树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样。

**剪枝的损失度量：** 对任意一颗子树 $T$ ，其损失函数为

$C\alpha(T_{t}) = C(T_{t}) + \alpha|T_{t}|$

$\alpha$ 是正则化系数，$C(T_{t})$ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是用均方差来度量，$|T_{t}|$ 是子树 T 的叶子节点数量。

当 $\alpha = 0$ 时，即没有正则化，原始的 $CART$ 树就是最优子树，当 $\alpha = \infty$，惩罚项达到最大，此时原始 $CART$ 树的根节点组成的树即是

最优子树。这是两种极端的情况，$\alpha$ 越大，剪枝的越厉害，最优子树就会越简单，如果固定 $\alpha$ ，那么一定有一个最优子树与其对应。

 **剪枝的思路：**

​	对于一颗子树 $T_{t}$ ，如果没有剪枝，它的损失是：

​	$C_{\alpha}(T_{t}) = C(T_{t}) + \alpha|T_{t}|$ 

​	如果将其剪掉，仅仅保留根节点，则损失是：

​	$C_{\alpha}(T) = C(T) + \alpha$

​	当 $\alpha$ 很小时，$C_{\alpha}(T_{t}) < C_{\alpha}(T)$ ，因为整个子树由于预测精准些，自然损失要比只有一个根节点的要小

​	当 $\alpha$ 慢慢增加，会有一个时刻有：

​	$C_{\alpha}(T_{t}) = C_{\alpha}(T)$

​	这时 $\alpha$ 为：

​	$\alpha = \frac{C(T)-C(T_{t})}{|T_{t}|-1}$ 

​	$T_{t}$ 和 $T$ 有相同的损失函数，但是 $T$ 更简单，结点更少，所以可以对 $T_{t}$ 剪枝，让其变为 $T$ 

​	综上，如果我们计算出每个子树是否剪枝的阈值 $\alpha$ ，然后对这些阈值求出最优子树，再使用交叉验证去选择最好的那棵子树最为最优子树。

**剪枝算法：**

1. 初始化 $k = 0, T = T_{0}$，最优子树集合 $ w = \{T\}$ 

2. $\alpha_{min} = \infty$

3. 从叶子结点自下而上的计算各内部结点 $t$ 的损失函数 $C_{\alpha}(T_{t})$，叶子结点数 $|T_{t}|$，以及正则化阈值 $\alpha = min\{ \frac{C(T)-C(T_{t})}{|T_{t}|-1},\alpha_{min}\}$ ，更新 $\alpha_{min} = \alpha$ 

4.   $\alpha_{k} = \alpha_{min}$ 

5. 从根节点自上而下访问子树 t 的内部节点，如果有 $\frac{C(T)-C(T_{t})}{|T_{t}|-1}\le \alpha_{k}$ ，进行剪枝，并决定叶节点 t 的预测值，分类树为 概率最大的类

   别，回归树为 均值，这样得到了 $\alpha_{k}$ 对应的最优子树 $T_{k}$

6. 最优子树集合 $w = w \cup T_{k}$

7. 进行下一轮剪枝，$k = k + 1,T = T_{k}$，如果 $T$ 不是由根节点组成的树，则回到步骤 2 继续递归的执行，否则就已经得到所有可选的最优子树集合 $w$ 

8. 采用交叉验证在 $w$ 选择最优子树 $T_{\alpha}$ 



## 7. 决策树优缺点

**优点：**

1. 简单直观，生成的决策树很直观。
2. 基本不需要预处理，不需要提前归一化，处理缺失值。
3. 使用决策树的预测代价为 $O(log_{2}m)$。
4. 既可以处理离散值也可以处理连续值。
5. 可以处理多维度输出的分类问题。
6. 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释。
7. 可以对决策树进行剪枝，提高泛化能力。
8. 对于异常点的容错能力好，健壮性高。

**缺点：**

1. 决策树算法非常容易过拟合，导致泛化能力不强，可以设置结点中最少样本数量和限制决策树的深度。
2. 决策树会因为样本的一点点的改动，就会导致树结构的剧烈变化，这个可以通过集成学习的方法来解决。
3. 寻找最优的决策树是一个 NP 问题，我们一般是通过启发式方式，容易陷入局部最优，可以通过集成学习的方法来改善。
4. 有些比较复杂的关系决策树很难学习。
5. 如果某些特征的样本比例过大，生成的决策树容易偏向于这些特征，可以调节样本权重来改善。 



















