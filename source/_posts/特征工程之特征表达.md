---
title: 特征工程之特征表达
tags: 机器学习
categories: 机器学习
mathjax: true
abbrlink: 7561
date: 2022-03-28 18:47:00
---

# 特征工程之特征表达

## 1. 缺失值处理

数据有缺失值是很常见的，但是在训练模型时，数据要求不能有缺失值，处理方法如下：

1. 针对连续值，可以取所有样本的平均值来填充，或者取所有样本的中位数来填充
2. 如果是离散值，可以取样本中出现最频繁的特征值来填充

## 2. 特殊的特征处理

有些特征的取值比较特殊，例如日期时间，该如何表达呢？

1. 时间差值法，计算所有样本时间到未来某一个时间的数值差距，这个差距是 UTC 的时间差，从而将时间特征转化为连续值
2. 根据时间所在的年、月、日、星期几，将一个时间特征转化为多个离散的特征，这种方法在分析具有明显时间趋势的问题上比较好用
3. 根据时间的新旧得到一个权重值，例如，三个月前是某个权重，最近一个月是一个权重，最近一周又是另一个权重

对地理特征的处理

1. 离散值，可以将一个位置处理成多个特征，比如，城市特征，区县特征，接到特征等
2. 连续值，但如果我们需要判断用户分布的区域，则一般处理成连续值会比较好，可以转化为地里的经度和纬度

## 3. 离散特征连续化处理

有很多机器学习算法只能处理连续特征，不能处理离散特征，比如线性回归，逻辑回归等。

1. 独热编码：这是最常见的处理方法，比如某特征的取值是高、中、低，那么我们就可以创建三个取值为 0 或者 1 的特征，将高编码为 1, 0, 0 这样三个特征，中编码为 0，1，0 这三个特征，低编码为 0，0，1 这三个特征，也就是说之前的一个特征被我们转化为三个特征，sklearn 中的 OneHotEncoder 可以进行独热编码。
2. 特征嵌入 embedding ，这个方法一般用于深度学习中，比如对于用户 ID 这个特征，如果使用独热编码，则维度会爆炸，如果使用特征嵌入维度就会低很多，对于每个要嵌入的特征，我们会有一个嵌入矩阵，这个矩阵的行很大，对应该特征的数目，比如用户 ID，如果有 100 万个，那么嵌入的特征矩阵的行就是100万个，但是列一般比较小，比如取 20，这样这个用户 ID 特征就转化为 20 维，进而参与深度学习模型，在 tensorflow 中，我们可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用 tf.nn.embedding_lookup找到该用户的特征嵌入向量，特征嵌入矩阵会在反向传播的迭代中优化。
3. 在自然语言处理中，也可以用 word2vec 将词转换为词向量，进而可以进行一些连续值的处理。

## 4. 离散特征的离散化处理

离散特征有时候也不能直接使用，需要进行转化

1. 上一节介绍的是独热编码，这一节的方法是 虚拟编码 dummy coding，它和独热编码类似，但是它的特点是，如果该特征有 N 个取值，只需要 N-1 个新的 0 和 1 的特征来代替，而独热编码则会用 N 个新特征来代替，例如 高、中、低，只需要两个特征来代替，00 代表高，10 代表中，01 代表低，一般还是使用独热编码。

2. 有的时候可以修改特征，例如 原始特征有 春夏秋冬，可以将其修改为 旺季和淡季这样的二值特征。也可以变成三值特征或四值特征。

3. 对于分类问题的输出，一般需要用 sklearn 的 LabelEncoder 将其转化为 $0,1,2\dots$ 这样的类别标签值。

## 5. 连续特征离散化处理

1. 根据阈值进行分组，例如年龄，0-20 分一类，20-40分一类，后面依次类推。
2. 使用 GBDT 方法，GBDT先将连续值转化为离散值，那么如何转化呢，例如，如果使用样本集中所有的连续值和标签来训练 GBDT，训练出的GBDT有两棵决策树，第一棵决策树有 三个 叶子节点，第二棵决策树有四个叶子节点，一个样本落在第一棵决策树的第二个叶子节点，落在第二棵决策树的第四个叶子节点，那么这个样本的连续值编码为离散值则为 （0，1，0，0，0，0，1）一共七个离散的特征，可以使用 sklearn 中的 GradientBoostingClassifier 的 apply 方法很方便的得到样本的离散化后的特征，然后使用独热编码即可。





















